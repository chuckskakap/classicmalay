{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRbGDwdlMG4F",
        "outputId": "b10ac4db-f69a-41be-fc33-d64e2c0442be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Collecting keras-nlp\n",
            "  Downloading keras_nlp-0.17.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Collecting keras-hub==0.17.0 (from keras-nlp)\n",
            "  Downloading keras_hub-0.17.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from keras-hub==0.17.0->keras-nlp) (2024.9.11)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras-hub==0.17.0->keras-nlp) (13.9.3)\n",
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.10/dist-packages (from keras-hub==0.17.0->keras-nlp) (0.3.3)\n",
            "Collecting tensorflow-text (from keras-hub==0.17.0->keras-nlp)\n",
            "  Downloading tensorflow_text-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kagglehub->keras-hub==0.17.0->keras-nlp) (4.66.6)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras-hub==0.17.0->keras-nlp) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras-hub==0.17.0->keras-nlp) (2.18.0)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting tensorboard<2.19,>=2.18 (from tensorflow)\n",
            "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting keras>=3.5.0 (from tensorflow)\n",
            "  Downloading keras-3.6.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras-hub==0.17.0->keras-nlp) (0.1.2)\n",
            "Downloading keras_nlp-0.17.0-py3-none-any.whl (2.0 kB)\n",
            "Downloading keras_hub-0.17.0-py3-none-any.whl (644 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.1/644.1 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_text-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.3/615.3 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-3.6.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorboard, keras, tensorflow, tensorflow-text, keras-hub, keras-nlp\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.0\n",
            "    Uninstalling tensorboard-2.17.0:\n",
            "      Successfully uninstalled tensorboard-2.17.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.4.1\n",
            "    Uninstalling keras-3.4.1:\n",
            "      Successfully uninstalled keras-3.4.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.0\n",
            "    Uninstalling tensorflow-2.17.0:\n",
            "      Successfully uninstalled tensorflow-2.17.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.18.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-3.6.0 keras-hub-0.17.0 keras-nlp-0.17.0 tensorboard-2.18.0 tensorflow-2.18.0 tensorflow-text-2.18.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow keras-nlp pandas numpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U tensorflow keras-nlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKJAXNSLPJGd",
        "outputId": "a484299b-79a2-4db8-bc93-10c4b4f9c7f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.18.0)\n",
            "Requirement already satisfied: keras-nlp in /usr/local/lib/python3.10/dist-packages (0.17.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.6.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: keras-hub==0.17.0 in /usr/local/lib/python3.10/dist-packages (from keras-nlp) (0.17.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from keras-hub==0.17.0->keras-nlp) (2024.9.11)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras-hub==0.17.0->keras-nlp) (13.9.3)\n",
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.10/dist-packages (from keras-hub==0.17.0->keras-nlp) (0.3.3)\n",
            "Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.10/dist-packages (from keras-hub==0.17.0->keras-nlp) (2.18.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow) (0.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kagglehub->keras-hub==0.17.0->keras-nlp) (4.66.6)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras-hub==0.17.0->keras-nlp) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras-hub==0.17.0->keras-nlp) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras-hub==0.17.0->keras-nlp) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import time\n",
        "import keras_nlp\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "# Function to load and preprocess data\n",
        "def load_data(gsheet_id, sheet_name):\n",
        "    \"\"\"Load data from Google Sheets and preprocess it\"\"\"\n",
        "    gsheet_url = f\"https://docs.google.com/spreadsheets/d/{gsheet_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}\"\n",
        "    try:\n",
        "        df = pd.read_csv(gsheet_url)\n",
        "        df = df.iloc[:, 0:3]  # Keep only first 3 columns\n",
        "\n",
        "        # Clean text content\n",
        "        df['content'] = (df['content'].str.lower()\n",
        "                        .str.replace('\\r\\n', ' ', regex=False)\n",
        "                        .str.replace('\\t', ' ', regex=False)\n",
        "                        .str.replace('\\n', ' ', regex=False)\n",
        "                        .str.replace('       ', ' ', regex=False)\n",
        "                        .str.strip())\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        return None\n",
        "\n",
        "def test_generation(model, prompts):\n",
        "    \"\"\"Test model generation with given prompts\"\"\"\n",
        "    print(\"\\nTesting generation...\")\n",
        "    for prompt in prompts:\n",
        "        print(f\"\\nPrompt: {prompt}\")\n",
        "        output = model.generate(prompt, max_length=200)\n",
        "        print(f\"Generated text: {output}\")\n",
        "\n",
        "def create_model(sequence_length=128):\n",
        "    \"\"\"Create and compile the GPT-2 model with preset configurations\"\"\"\n",
        "    # Load the preset tokenizer and preprocessor\n",
        "    preset_name = \"gpt2_base_en\"\n",
        "\n",
        "    # Initialize preprocessor with preset\n",
        "    preprocessor = keras_nlp.models.GPT2CausalLMPreprocessor.from_preset(\n",
        "        preset_name,\n",
        "        sequence_length=sequence_length\n",
        "    )\n",
        "\n",
        "    # Create backbone from preset\n",
        "    backbone = keras_nlp.models.GPT2Backbone.from_preset(preset_name)\n",
        "\n",
        "    # Initialize model with backbone and preprocessor\n",
        "    model = keras_nlp.models.GPT2CausalLM(\n",
        "        backbone=backbone,\n",
        "        preprocessor=preprocessor\n",
        "    )\n",
        "\n",
        "    # Compile model\n",
        "    learning_rate = keras.optimizers.schedules.PolynomialDecay(\n",
        "        5e-4,\n",
        "        decay_steps=500 * 15,  # adjust based on your dataset size and epochs\n",
        "        end_learning_rate=0.0,\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate),\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        weighted_metrics=[\"accuracy\"],\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "def prepare_dataset(texts, batch_size=8):\n",
        "    \"\"\"Prepare dataset for training\"\"\"\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(texts)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.cache()\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "    return dataset.take(500)  # Limiting to 500 batches as in original code\n",
        "\n",
        "def save_model(model, base_path=\"./\"):\n",
        "    \"\"\"Save model weights and configuration\"\"\"\n",
        "    try:\n",
        "        # Create directory if it doesn't exist\n",
        "        Path(base_path).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Save full model with .keras extension\n",
        "        model.save(f\"{base_path}/model.keras\")\n",
        "\n",
        "        # Save weights separately\n",
        "        weights_path = Path(base_path) / \"model_weights\"\n",
        "        weights_path.mkdir(exist_ok=True)\n",
        "        model.save_weights(f\"{weights_path}/weights.keras\")\n",
        "\n",
        "        print(f\"Model saved successfully to {base_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving model: {e}\")\n",
        "\n",
        "        # Try alternative h5 format if keras format fails\n",
        "        try:\n",
        "            print(\"Attempting to save in h5 format...\")\n",
        "            model.save(f\"{base_path}/model.h5\")\n",
        "            model.save_weights(f\"{weights_path}/weights.h5\")\n",
        "            print(\"Model saved successfully in h5 format\")\n",
        "        except Exception as e2:\n",
        "            print(f\"Error saving in h5 format: {e2}\")\n",
        "\n",
        "def main():\n",
        "    # Configuration\n",
        "    GSHEET_ID = \"191YRBsdUEGtgXvWl428L9xt48Ah5zcyoRK1pvNhUDPM\"\n",
        "    SHEET_NAME = \"Sheet2\"\n",
        "    NUM_EPOCHS = 15\n",
        "\n",
        "    # Test prompts\n",
        "    test_prompts = [\n",
        "        \"My trip to Kuala Lumpur was so annoying\",\n",
        "        \"raja dan permaisuri\"\n",
        "    ]\n",
        "\n",
        "    # Create base model and test it first\n",
        "    print(\"Creating base model...\")\n",
        "    base_model = keras_nlp.models.GPT2CausalLM.from_preset(\"gpt2_base_en\")\n",
        "\n",
        "    print(\"\\nTesting base model (before fine-tuning):\")\n",
        "    test_generation(base_model, test_prompts)\n",
        "\n",
        "    # Load and preprocess data\n",
        "    print(\"\\nLoading data for fine-tuning...\")\n",
        "    df = load_data(GSHEET_ID, SHEET_NAME)\n",
        "    if df is None:\n",
        "        return\n",
        "\n",
        "    # Create and compile model for fine-tuning\n",
        "    print(\"\\nCreating model for fine-tuning...\")\n",
        "    model = create_model()\n",
        "\n",
        "    # Prepare dataset\n",
        "    print(\"Preparing dataset...\")\n",
        "    train_ds = prepare_dataset(df['content'])\n",
        "\n",
        "    # Train model\n",
        "    print(\"Starting fine-tuning...\")\n",
        "    start_time = time.time()\n",
        "    model.fit(train_ds, epochs=NUM_EPOCHS)\n",
        "    end_time = time.time()\n",
        "    print(f\"Training completed in {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "    # Test fine-tuned model\n",
        "    print(\"\\nTesting fine-tuned model:\")\n",
        "    test_generation(model, test_prompts)\n",
        "\n",
        "    # Save model\n",
        "    print(\"\\nSaving fine-tuned model...\")\n",
        "    save_model(model)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOXaftpxOy2U",
        "outputId": "550708a2-0449-4f1d-e0ea-acb6a931d229"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating base model...\n",
            "\n",
            "Testing base model (before fine-tuning):\n",
            "\n",
            "Testing generation...\n",
            "\n",
            "Prompt: My trip to Kuala Lumpur was so annoying\n",
            "Generated text: My trip to Kuala Lumpur was so annoying, I decided to go with my friend to a local hotel. I was very nervous, but I didn't have any issues. We arrived at the hotel in a hurry, and it's a very nice hotel, so I went to check on my luggage, but the hotel was a little bit crowded. I didn't want to wait too long, so I went with my friend. I got to the hotel and I saw the room that was next to the hotel and it was very comfortable. I was happy and I didn't feel like I was in a bad mood. I went in the lobby with my friend and we were very happy with the room. We went out to the bathroom and I was very pleased with how comfortable it was, but it wasn't very comfortable for my body. It was so hot, so hot, I didn't even think about it. I didn't want to get sweaty and I didn't feel like I was sweaty\n",
            "\n",
            "Prompt: raja dan permaisuri\n",
            "Generated text: raja dan permaisuri permai permai permai permai permai permai permai permai permai permai permai permai permai permai permai permai permai permai permai permai permai permai permai permai permai permai permai permai permai permai permai permai permai permai permai permai permai permai permai permai permai permai permai permai permai permai permai permai permai permai permai permai permai permai permai permai permai permai permai permai permai permai permai permai\n",
            "\n",
            "Loading data for fine-tuning...\n",
            "\n",
            "Creating model for fine-tuning...\n",
            "Preparing dataset...\n",
            "Starting fine-tuning...\n",
            "Epoch 1/15\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 4s/step - accuracy: 0.0751 - loss: 6.0104\n",
            "Epoch 2/15\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 343ms/step - accuracy: 0.1918 - loss: 4.2904\n",
            "Epoch 3/15\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 345ms/step - accuracy: 0.2654 - loss: 3.6082\n",
            "Epoch 4/15\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 313ms/step - accuracy: 0.3252 - loss: 3.1003\n",
            "Epoch 5/15\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 344ms/step - accuracy: 0.3904 - loss: 2.6790\n",
            "Epoch 6/15\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 360ms/step - accuracy: 0.4477 - loss: 2.3077\n",
            "Epoch 7/15\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 341ms/step - accuracy: 0.5137 - loss: 1.9575\n",
            "Epoch 8/15\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 338ms/step - accuracy: 0.5688 - loss: 1.6692\n",
            "Epoch 9/15\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 309ms/step - accuracy: 0.6307 - loss: 1.3996\n",
            "Epoch 10/15\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 340ms/step - accuracy: 0.6835 - loss: 1.1648\n",
            "Epoch 11/15\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 341ms/step - accuracy: 0.7471 - loss: 0.8895\n",
            "Epoch 12/15\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 349ms/step - accuracy: 0.7990 - loss: 0.6988\n",
            "Epoch 13/15\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 346ms/step - accuracy: 0.8611 - loss: 0.4820\n",
            "Epoch 14/15\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 345ms/step - accuracy: 0.8829 - loss: 0.4058\n",
            "Epoch 15/15\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 342ms/step - accuracy: 0.9060 - loss: 0.3377\n",
            "Training completed in 201.64 seconds\n",
            "\n",
            "Testing fine-tuned model:\n",
            "\n",
            "Testing generation...\n",
            "\n",
            "Prompt: My trip to Kuala Lumpur was so annoying\n",
            "Generated text: My trip to Kuala Lumpur was so annoying. nirnama sendiri tidak berhenti bersungut sampai ke pergunungan ini. mujur kedinginan menyambut tangan kucar-kacir semuanya. laluannya. semua itu bertukar hanya.  “ampun. kesejukan sudah begitu megah. mujur kesejukan sudah makanan kakinya. mamanda bendahara semuanya.“masih berharim\n",
            "\n",
            "Prompt: raja dan permaisuri\n",
            "Generated text: raja dan permaisuri pergunungan ini. rumput-rumput yang berbentuk bulatan. perlahanusan-lahan dia menebarkan pandangan. dia kemudian seperti sebelumnya. bahkan. dia bercerita. apabila dia bah. dia berada di dia dibelai. bau masih berbau asin air lain serendah. lagi dia berharap nirnama.\n",
            "\n",
            "Saving fine-tuned model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error saving model: The filename must end in `.weights.h5`. Received: filepath=model_weights/weights.keras\n",
            "Attempting to save in h5 format...\n",
            "Error saving in h5 format: The filename must end in `.weights.h5`. Received: filepath=model_weights/weights.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze > requirements.txt"
      ],
      "metadata": {
        "id": "KQHJsbataUzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import keras_nlp\n",
        "from pathlib import Path\n",
        "\n",
        "def load_saved_model(model_path=\"./model.keras\"):\n",
        "    \"\"\"\n",
        "    Load a saved GPT-2 model.\n",
        "    Args:\n",
        "        model_path: Path to the saved model file\n",
        "    Returns:\n",
        "        Loaded model ready for inference\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load the full model\n",
        "        model = keras.models.load_model(model_path)\n",
        "        print(f\"Model successfully loaded from {model_path}\")\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model from {model_path}: {e}\")\n",
        "\n",
        "        # Try loading from h5 format if keras format fails\n",
        "        try:\n",
        "            h5_path = str(Path(model_path).parent / \"model.h5\")\n",
        "            print(f\"Attempting to load from h5 format: {h5_path}\")\n",
        "            model = keras.models.load_model(h5_path)\n",
        "            print(\"Model successfully loaded from h5 format\")\n",
        "            return model\n",
        "        except Exception as e2:\n",
        "            print(f\"Error loading from h5 format: {e2}\")\n",
        "            return None\n",
        "\n",
        "def generate_text(model, prompt, max_length=200, temperature=0.7):\n",
        "    \"\"\"\n",
        "    Generate text using the loaded model.\n",
        "    Args:\n",
        "        model: Loaded GPT-2 model\n",
        "        prompt: Input text to generate from\n",
        "        max_length: Maximum length of generated text\n",
        "        temperature: Controls randomness (higher = more random)\n",
        "    Returns:\n",
        "        Generated text\n",
        "    \"\"\"\n",
        "    try:\n",
        "        output = model.generate(\n",
        "            prompt,\n",
        "            max_length=max_length,\n",
        "            temperature=temperature\n",
        "        )\n",
        "        return output\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating text: {e}\")\n",
        "        return None\n",
        "\n",
        "# Example usage\n",
        "def main():\n",
        "    # Load the model\n",
        "    model = load_saved_model()\n",
        "    if model is None:\n",
        "        return\n",
        "\n",
        "    # Test some prompts\n",
        "    test_prompts = [\n",
        "        \"My trip to Kuala Lumpur was so annoying\",\n",
        "        \"raja dan permaisuri\",\n",
        "        \"The weather in Malaysia is\"\n",
        "    ]\n",
        "\n",
        "    print(\"\\nGenerating text from test prompts:\")\n",
        "    for prompt in test_prompts:\n",
        "        print(f\"\\nPrompt: {prompt}\")\n",
        "        generated_text = generate_text(model, prompt)\n",
        "        print(f\"Generated: {generated_text}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKmTw5wif8m5",
        "outputId": "03cd2ac4-011a-4e9a-ff05-fb945b0ebce3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/saving/serialization_lib.py:734: UserWarning: `compile()` was not called as part of model loading because the model's `compile()` method is custom. All subclassed Models that have `compile()` overridden should also override `get_compile_config()` and `compile_from_config(config)`. Alternatively, you can call `compile()` manually after loading.\n",
            "  instance.compile_from_config(compile_config)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 393 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model successfully loaded from ./model.keras\n",
            "\n",
            "Generating text from test prompts:\n",
            "\n",
            "Prompt: My trip to Kuala Lumpur was so annoying\n",
            "Error generating text: CausalLM.generate() got an unexpected keyword argument 'temperature'\n",
            "Generated: None\n",
            "\n",
            "Prompt: raja dan permaisuri\n",
            "Error generating text: CausalLM.generate() got an unexpected keyword argument 'temperature'\n",
            "Generated: None\n",
            "\n",
            "Prompt: The weather in Malaysia is\n",
            "Error generating text: CausalLM.generate() got an unexpected keyword argument 'temperature'\n",
            "Generated: None\n"
          ]
        }
      ]
    }
  ]
}